{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlW1QalFjNCk6JrDRPafUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsandipan5/PracticePyspark/blob/main/18thAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dgi_gjpPAiM",
        "outputId": "d221f31c-169c-4244-8c37-26bee52bed3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 54.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=7230a3d335c2c006ea8707ff1db0462b6c3afa70a7972c81e50336e438a5c410\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pyspark Create RDD example\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()"
      ],
      "metadata": {
        "id": "awuthyctPGUe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team = spark.createDataFrame([('1','Sandipan','BigData','4thfloor'),\n",
        "                             ('2','Saikat','MDM','3thfloor')],['Slno','Name','Dept','Floor'])\n",
        "team.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "varyUnFIPg2r",
        "outputId": "61bde2c5-a63d-497d-c679-1964cd461caa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-------+--------+\n",
            "|Slno|    Name|   Dept|   Floor|\n",
            "+----+--------+-------+--------+\n",
            "|   1|Sandipan|BigData|4thfloor|\n",
            "|   2|  Saikat|    MDM|3thfloor|\n",
            "+----+--------+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "team1 = spark.createDataFrame([('1','Sandipan','BigData','4thfloor'),\n",
        "                             ('2','Shyam','MDM','3thfloor')],['Slno','Name','Dept','Floor'])\n",
        "team1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIYUoI5aQLim",
        "outputId": "440d6fac-283a-4c44-8646-e97580008d59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+-------+--------+\n",
            "|Slno|    Name|   Dept|   Floor|\n",
            "+----+--------+-------+--------+\n",
            "|   1|Sandipan|BigData|4thfloor|\n",
            "|   2|   Shyam|    MDM|3thfloor|\n",
            "+----+--------+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"PsarkParallize\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
        "df1 = spark.sparkContext.parallelize ([\n",
        "(12, 20, 35, 'a b c'),\n",
        "(41, 34, 64, 'd e f'),\n",
        "(70, 85, 68, 'g e f')]).toDF(['1', '2', '3','4'])\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGRBFQJwUsz6",
        "outputId": "d68d9273-b5e6-4f20-d395-3f319e402ad1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+-----+\n",
            "|  1|  2|  3|    4|\n",
            "+---+---+---+-----+\n",
            "| 12| 20| 35|a b c|\n",
            "| 41| 34| 64|d e f|\n",
            "| 70| 85| 68|g e f|\n",
            "+---+---+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dv4etwegUso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "mtUmTGyuUr-6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyz = sc.parallelize(                                                                                                                                             \n",
        "[\"scala\",                                                                                                                                                         \n",
        "\"java\",                                                                                                                                                           \n",
        "\"hadoop\",                                                                                                                                                         \n",
        "\"spark vs hadoop\",                                                                                                                                                \n",
        " \"pyspark\",                                                                                                                                                        \n",
        "\"sparkconf\"]) "
      ],
      "metadata": {
        "id": "soYrUbuiTfTS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kpr2cb0UjT-",
        "outputId": "498e8f82-4560-46e5-f284-d12df8e9bf37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnVUd-S9VC4W",
        "outputId": "fbd0cb8d-94c8-4ce2-dc5e-8f0ef864073d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala', 'java', 'hadoop']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8vGDvFiEGKC",
        "outputId": "82f0aa6d-391e-4ff9-c3cb-8cbbfe51b8b3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala', 'java', 'hadoop', 'spark vs hadoop', 'pyspark', 'sparkconf']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EeckNdEPAn",
        "outputId": "10424f84-dba9-41ff-ebd1-b3b1b5b19937"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[28] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xyz.saveAsTextFile('new.txt')"
      ],
      "metadata": {
        "id": "kevNjfGcEUVp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyz1 = xyz.repartition(4)"
      ],
      "metadata": {
        "id": "ZZOEdNc1ElHc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xyz1.saveAsTextFile('new1.txt')"
      ],
      "metadata": {
        "id": "1bvxBJY-E4At"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame.from_dict(\n",
        "        {\n",
        "            'Name':['Virat','Rohit','ABD','McCullum'],\n",
        "            'Rank':[1,2,3,4],\n",
        "            'Country':['India','India','SA','NZ']\n",
        "        }\n",
        ")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsNAOzIWFIm9",
        "outputId": "7cab3944-7ff2-4993-dd5b-31ca561273e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Name  Rank Country\n",
            "0     Virat     1   India\n",
            "1     Rohit     2   India\n",
            "2       ABD     3      SA\n",
            "3  McCullum     4      NZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime,date\n",
        "from pyspark.sql import Row\n",
        "import pandas as pd\n",
        "df = spark.createDataFrame([\n",
        "      Row(a=1, b=2, c='string1', d=date(2022,12,30), e = datetime(2022,1,4, 12,3,59)),\n",
        "      Row(a=2, b=2, c='string1', d=date(2022,5,30), e = datetime(2022,1,1, 12,39,40)),\n",
        "      Row(a=3, b=2, c='string1', d=date(2024,2,29), e = datetime(2022,1,1, 12,0,15)),\n",
        "])\n",
        "df.show()\n",
        "df.show(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZqGWg2_Rp8-",
        "outputId": "02ad4833-33c1-463f-8663-e3ba2af75ecb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|string1|2022-12-30|2022-01-04 12:03:59|\n",
            "|  2|  2|string1|2022-05-30|2022-01-01 12:39:40|\n",
            "|  3|  2|string1|2024-02-29|2022-01-01 12:00:15|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|string1|2022-12-30|2022-01-04 12:03:59|\n",
            "|  2|  2|string1|2022-05-30|2022-01-01 12:39:40|\n",
            "+---+---+-------+----------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(\n",
        "[\n",
        " (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,11,50,23)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0))\n",
        "\n",
        "\n",
        "\n",
        "]   \n",
        ")\n",
        "\n",
        "df = spark.createDataFrame(rdd, schema = ['a','b','c','d','e'])\n",
        "df\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWSGTxfoerrv",
        "outputId": "2c1fde34-a9e2-47b2-9cf6-d49c166bee97"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 11:50:23|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = spark.sparkContext.parallelize(\n",
        "[\n",
        " (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,11,50,23)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0))]).toDF(['1','2','3','4','5'])\n"
      ],
      "metadata": {
        "id": "xBYtBTuPiTdK"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = spark.sparkContext.parallelize(\n",
        "[\n",
        " (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,11,50,23)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0)),\n",
        "  (1,2., 'string1', date(2022,1,1), datetime(2022,1,1,12,0))])\n"
      ],
      "metadata": {
        "id": "UYGG_X1LkE89"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngQ9JIZIiacK",
        "outputId": "19ba57e4-26c2-40f0-dfad-2b7165728279"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  1|  2|      3|         4|                  5|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 11:50:23|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 =  spark.createDataFrame(rdd2,schema=['a','b','c','d','e'])\n",
        "rdd3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olEvRDqxi4De",
        "outputId": "60f03f6e-b87c-4c12-f260-4498d4b868c4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+\n",
            "|  a|  b|      c|         d|                  e|\n",
            "+---+---+-------+----------+-------------------+\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 11:50:23|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|\n",
            "+---+---+-------+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Column\n",
        "from pyspark.sql.functions import lower,upper,initcap\n",
        "\n",
        "#type(df.c) == type(upper(df.c)) == type(df.c.isNull())\n",
        "\n",
        "df.withColumn('InitialCap', initcap(df.c)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeKdX6fikAyH",
        "outputId": "36182634-1456-4be1-dc7d-d50b334ba9f7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+-------+----------+-------------------+----------+\n",
            "|  a|  b|      c|         d|                  e|InitialCap|\n",
            "+---+---+-------+----------+-------------------+----------+\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 11:50:23|   String1|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|   String1|\n",
            "|  1|2.0|string1|2022-01-01|2022-01-01 12:00:00|   String1|\n",
            "+---+---+-------+----------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(upper(df.c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrlkXt1wrRKx",
        "outputId": "d27b2fea-a33e-4f43-9d90-311d433eadd3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.column.Column"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHvfzxYTrbMr",
        "outputId": "5a7f22a2-5f29-4dc6-b43e-bb1d611c3d4b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.column.Column"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df.c.isNull())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvK3hGt4slM1",
        "outputId": "9eeefaee-1eef-4117-ae24-97edb3a93011"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.column.Column"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "BhKpnXOKspxZ",
        "outputId": "d3279c79-f962-466c-f304-4b93976e09d3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fca9f25e710>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://26d08ec4f4fb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6ib5dKArsxTX"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}